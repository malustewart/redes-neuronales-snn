#import "@preview/touying:0.5.5": *
#import "@preview/clean-math-presentation:0.1.1": *
#import "@preview/subpar:0.2.2"
#import "@preview/fletcher:0.5.3": diagram, node, edge
#import "@preview/dashy-todo:0.1.3": todo

#let todo-inline(content) = {
  todo(position: "inline", stroke: rgb(255,0,0))[#content]
}

#set grid(gutter: 10pt)

#show: clean-math-presentation-theme.with(
  config-info(
    title: [Spiking Neural Networks],
    short-title: [Spiking Neural Networks],
    authors: (
      (name: "Mar칤a Luz Stewart Harris")),
    author: "Mar칤a Luz Stewart Harris",
    date: datetime(year: 2025, month: 11, day: 26),
  ),
  config-common(
    slide-level: 3,
    // handout: true,
    // show-notes-on-second-screen: right,
  ),
  progress-bar: false,
  align: horizon
)

#title-slide(
  logo1: image("figs/Instituto-Balseiro.png", height: 4.5em),
)

#slide()[
    *#outline(title: [])*
]

= 쯈u칠 es una SNN?
#new-section-slide([], title: [])

#slide(title: "Spiking Neural Networks")[
  #definition(title: "Spiking Neural Networks / SNN", blocktitle: "Definici칩n")[
    Redes neuronales artificiales en el que el 칰nico intercambio que ocurre entre neuronas es el de pulsos de igual amplitud en diferentes instantes. @Maass_1997
  ]
  #pause
  - _*Inspiraci칩n de redes biol칩gicas*_: la comunicaci칩n entre neuronas biol칩gicas sucede a trav칠s de se침ales el칠ctricas que contienen pulsos: \
  #align(center, image("figs/Example_Vm.PNG", width: 80%))
]

#slide(title: "Spiking Neural Networks")[
  - En una SNN, la informaci칩n se codifica exclusivamente por la posici칩n de los pulsos transmitidos.
  - La amplitud y la forma de los pulsos *_no_* aportan m치s informaci칩n.
]

== Neuronas de una SNN

#slide()[
_*Caracter칤sticas necesarias de las neuronas de una SNN:*_][
    #pause
    - *_Memoria_*: Su salida en $t=t_0$ depende de la entrada en $t<=t_0$.#footnote[Aplica a neuronas individuales, indpendientemente de si la arquitectura de la red implementa memoria a trav칠s de recurrencia.]
    #pause
    - *_Event-driven_*: no transmiten informaci칩n todo el tiempo, solamente cuando ocurri칩 un evento.
    #pause
    - *_Async_*: generan un disparo apenas detectan un evento, sin tener que esperar a otras neuronas.
]

#slide(title: "Leaky Integrate-and-Fire")[
  - Existen muchos modelos matem치ticos de neuronas apropiados para una SNN, entre ellos el modelo _Leaky Integrate-and-Fire_ (LIF).
  #grid(align: (center+horizon, center+horizon), columns: (1fr, 1fr),
    [
    $
      cases(
        dot(s)(t) = 1/tau_s [s_("rest")-s(t)] + x(t),
        y(t) = sum_i^oo delta(t-t_f_i),
        s(t_(f_i) ^+) = s_("rest") ,
      )
    $],
    [
    #image("figs/LIF.PNG", height: 50%)
    ]
  )
]

== Codificaci칩n de informaci칩n en una SNN

#slide()[
    *_M칠todos de codificaci칩n de est칤mulo de entrada en pulsos:_*
][
    - _*Rate encoding*_: Convierte intensidad del est칤mulo de entrada en una tasa de disparos.
    - _*Temporal encoding*_: Convierte intensidad del est칤mulo de entrada en un tiempo de disparo.
    - _*Delta modulation*_: Convierte _cambios_ de intensidad del est칤mulo de entrada en disparos
]

#slide(align:center+horizon)[
    #image("figs/spike_encoding.png", width: 80%)
    @snntorch_2023
]

= 쮻칩nde se pueden ejecutar las SNNs?

#new-section-slide([], title:[])

== Hardware neurom칩rfico
#slide()[
    - Hardware para procesamiento de datos.
    #pause
    - A diferencia de hardware "tradicional" (CPU/GPU), las din치micas de las neuronas est치n implementadas por circuitos electr칩nicos#footnote([Existen l칤neas de investigaci칩n alternativas a la electr칩nica, como por ejemplo fot칩nica.]) y no por software.
    #pause
    - No ejecutan software. El 칰nico control disponible es el peso de conexiones entre neuronas.
]

=== Ejemplo de hardware neurom칩rfico: neurona l치ser

#slide(align: center+horizon, composer: (1fr, 2fr, 2fr))[
    #image("figs/VCSEL.png")\
][
    #image("figs/pulso_QIG_nahmias.PNG")\
    @Nahmias_Shastri_Tait_Prucnal_2013
][
    #image("figs/LIF.PNG")\
]

=== Ejemplo de hardware neurom칩rfico: procesador TrueNorth
#slide()[
    #image("figs/truenortharch.png")
][
    Procesador TrueNorth:
    #pause
    - Procesador neurom칩rfico para SNNs.
    #pause
    - 1 mill칩n de neuronas LIF.
    #pause
    - 256 millones de conexiones entre neuronas (sinapsis).
    #pause
    - Pesos de 2 bits de precisi칩n.
    #pause
    - Dividido en 4096 cores.
    #pause
    - Salida binaria de neuronas (_"digital en amplitud"_).
    #pause
    - Actualizaci칩n asincr칩nica de neuronas (_"anal칩gico en tiempo"_).
    #pause
    - Opcional: estocasticidad en la salida de las neuronas.
]


= 쯇or qu칠 usar SNNs?
#new-section-slide([], title: [])

#slide()[
    - _*Eficiencia energ칠tica*_: en muchos casos, el consumo de potencia es mucho menor que otras ANN principalmente debido a que:
        #pause
        - son _event-driven_: las neuronas solo transmiten informaci칩n (i.e.: un pulso) cuando ocurre un evento.
        #pause
        - las se침ales de salida son poco densas (_sparse_) debido a que, para cada neurona, la mayor parte del tiempo no hay eventos.#footnote[Esto depende del entrenamiento, pero es posible incluir la cantidad de disparos total de la red como factor en la funci칩n de costo para regular el consumo de potencia total de la red.]
    #pause
    - _*Baja latencia*_:
        #pause
        - Como son _event-driven_, los eventos se propagan en el momento que ocurren.
        #pause
        - Como se comunican 칰nicamente con pulsos, todos los mensajes transmitidos son muy cortos.
]
#slide()[
    - _*Apropiadas para procesamiento series temporales*_: 
        - Como las neuronas tienen memoria de su entrada en el pasado, no hace falta una arquitectura recurrente para procesar temporales.
    #pause
    - _*Similitud a redes neuronales biol칩gicas*_: son m치s cercanas a las redes neuronales biol칩gicas, lo que puede ser 칰til para modelar y estudiar su comportamiento.
]

== 쯈u칠 es una c치mara de eventos?
#focus-slide[Caso de uso: procesamiento de salida de c치maras de eventos.]

#slide(title:"쯈u칠 es una c치mara de eventos?", composer: (1.3fr, 1fr))[
    #image("figs/camera_comparison.png")
][
    - C치maras inspiradas en sistemas visuales biol칩gicos:
        #pause
        - Mayor sensibilidad a grandes cambios de intensidad que a intensidad constante.
        #pause
        - _Event-driven_: solo se guarda la informaci칩n de eventos (cambios). Donde no hay cambios no se guarda informaci칩n.
]

#slide(title:"쯈u칠 es una c치mara de eventos?", composer: (1.3fr, 1fr))[
    #image("figs/camera_comparison.png")
][
    - La salida es un arreglo de estructuras iguales con 3 datos (no se guardan frames completos!):
        - Posici칩n del evento
        - Timestamp del evento:
        - Polaridad del evento
    #pause
    #sym.arrow.double Comparada con una c치mara de frames convencional, tiene:
    - Menos volumen de datos de salida
    - M치s detalle del movimiento
]

#slide(align:center+horizon, composer: (15fr, 1fr))[
    #image("figs/frame_vs_event_based_dataset.png", height: 80%)][
    @gesture_recognition_2017
]

#slide(align:center+horizon, composer: (15fr, 1fr))[
    #image("figs/frame_vs_event_based_rocks.png")][
    @Hendy_Merkel_2022
]

== Procesamiento de salida de una c치mara de eventos con SNNs
#slide(align: center+horizon)[
    #text(size: 24pt)[Por el formato de los datos de salida de una c치mara de eventos,\
    las SNNs son ideales para procesarlos!]
]
#slide(align:center+horizon, composer: (15fr, 1fr))[
    #image("figs/gesture_recognition_arch.png")][
    @gesture_recognition_2017
]

#slide[
    #todo-inline[Explicar la parte de SNN del paper]
]

#slide[
    - Resultados:
        - Latencia promedio de 105ms en reconocimiento de gestos (percibido como en tiempo real por un humano)
        - Accuracy de entre 87.62% (para consumo de 88.5mW) y 96.44% (para consumo de 178.8mW) 
            #pause
            - #sym.arrow Un cargador de notebook de 60W alcanza para alimentar el an치lisis de 300 streams de video en tiempo real!
]

= 쮺칩mo se entrenan las SNNs?
#new-section-slide([], title: [])

#slide[
  #todo-inline[supervisado vs no supervisado (probablemente volar)]
  #todo-inline[supervisado: backpropagation y gradient descent, desafio de la no derivabilidad]
]

// = Referencias
#slide()[
  #bibliography("refs.bib", title: [Referencias])
]

#focus-slide[Muchas gracias! 游]


#show: appendix
#set heading(outlined: false)

= Leaky Integrate-and-Fire

#slide(title: "Leaky Integrate-and-Fire")[
  #definition(title: "Modelo Leaky Integrate-and-Fire (LIF)", blocktitle:"Definici칩n")[
  $
    cases(
      dot(s)(t) = 1/tau_s [s_("rest")-s(t)] + x(t),
      y(t) = sum_i^oo delta(t-t_f_i),
      s(t_(f_i) ^+) = s_("rest") ,
    )
  $ <eq:LIF>
  - $t_f_i$: $t$ tal que $s(t) >= s_(t h),$
  - $x(t)$: entrada (pulsos)
  - $s(t)$: estado de la neurona
  - $y(t)$: tren de deltas (disparos)
  ]
]

