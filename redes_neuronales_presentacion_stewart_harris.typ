#import "@preview/touying:0.5.5": *
#import "@preview/clean-math-presentation:0.1.1": *
#import "@preview/subpar:0.2.2"
#import "@preview/fletcher:0.5.3": diagram, node, edge
#import "@preview/dashy-todo:0.1.3": todo

#let todo-inline(content) = {
  todo(position: "inline", stroke: rgb(255,0,0))[#content]
}

#set grid(gutter: 10pt)

#show: clean-math-presentation-theme.with(
  config-info(
    title: [Spiking Neural Networks],
    short-title: [Spiking Neural Networks],
    authors: (
      (name: "Mar铆a Luz Stewart Harris")),
    author: "Mar铆a Luz Stewart Harris",
    date: datetime(year: 2025, month: 11, day: 26),
  ),
  config-common(
    slide-level: 3,
    // handout: true,
    // show-notes-on-second-screen: right,
  ),
  progress-bar: false,
  align: horizon
)

#title-slide(
  logo1: image("figs/Instituto-Balseiro.png", height: 4.5em),
)

= 驴Qu茅 es una SNN?
#new-section-slide([], title: [])

#slide(title: "Spiking Neural Networks")[
  #definition(title: "Spiking Neural Networks / SNN", blocktitle: "Definici贸n")[
    Redes neuronales artificiales en el que el 煤nico intercambio que ocurre entre neuronas es el de pulsos de igual amplitud en diferentes instantes. @Maass_1997
  ]
  #pause
  - _*Inspiraci贸n de redes biol贸gicas*_: la comunicaci贸n entre neuronas biol贸gicas sucede a trav茅s de se帽ales el茅ctricas que contienen pulsos: \
  #align(center, image("figs/Example_Vm.PNG", width: 80%))
]

#slide(title: "Spiking Neural Networks")[
  - En una SNN, la informaci贸n se codifica exclusivamente por la posici贸n de los pulsos transmitidos.
  - La amplitud y la forma de los pulsos *_no_* aportan m谩s informaci贸n.
]

== Neuronas de una SNN

#slide()[
_*Caracter铆sticas necesarias de las neuronas de una SNN:*_][
    #pause
    - *_Memoria_*: Su salida en $t=t_0$ depende de la entrada en $t<=t_0$.#footnote[Aplica a neuronas individuales, indpendientemente de si la arquitectura de la red implementa memoria a trav茅s de recurrencia.]
    #pause
    - *_Event-driven_*: no transmiten informaci贸n todo el tiempo, solamente cuando ocurri贸 un evento.
    #pause
    - *_Async_*: generan un disparo apenas detectan un evento, sin tener que esperar a otras neuronas.
]

#slide(title: "Leaky Integrate-and-Fire")[
  - Existen muchos modelos matem谩ticos de neuronas apropiados para una SNN, entre ellos el modelo _Leaky Integrate-and-Fire_ (LIF).
  #grid(align: (center+horizon, center+horizon), columns: (1fr, 1fr),
    [
    $
      cases(
        dot(s)(t) = 1/tau_s [s_("rest")-s(t)] + x(t),
        y(t) = sum_i^oo delta(t-t_f_i),
        s(t_(f_i) ^+) = s_("rest") ,
      )
    $],
    [
    #image("figs/LIF.PNG", height: 50%)
    ]
  )
]

== Codificaci贸n de informaci贸n en una SNN

#slide()[
    *_M茅todos de codificaci贸n de est铆mulo de entrada en pulsos:_*
][
    #pause
    - _*Rate encoding*_: Convierte intensidad del est铆mulo de entrada en una tasa de disparos.
    #pause
    - _*Temporal encoding*_: Convierte intensidad del est铆mulo de entrada en un tiempo de disparo.
    #pause
    - _*Delta modulation*_: Convierte _cambios_ de intensidad del est铆mulo de entrada en disparos
]

#slide(align:center+horizon)[
    #image("figs/spike_encoding.png", width: 80%)
    @snntorch_2023
]

= 驴D贸nde se pueden ejecutar las SNNs?

#new-section-slide([], title:[])

== Hardware neurom贸rfico
#slide()[
    - Hardware para procesamiento de datos.
    #pause
    - A diferencia de hardware "tradicional" (CPU/GPU), las din谩micas de las neuronas est谩n implementadas por circuitos electr贸nicos#footnote([Existen l铆neas de investigaci贸n alternativas a la electr贸nica, como por ejemplo fot贸nica.]) y no por software.
    #pause
    - No ejecutan software. El 煤nico control disponible es el peso de conexiones entre neuronas.
]

=== Ejemplo de hardware neurom贸rfico: neurona l谩ser

#slide(align: center+horizon, composer: (1fr, 2fr, 2fr))[
    #image("figs/VCSEL.png")\
][
    #image("figs/pulso_QIG_nahmias.PNG")\
    @Nahmias_Shastri_Tait_Prucnal_2013
][
    #image("figs/LIF.PNG")\
]

=== Ejemplo de hardware neurom贸rfico: procesador TrueNorth
#slide()[
    #image("figs/truenortharch.png", height: 80%)
][
    Procesador TrueNorth:
    #pause
    - Procesador neurom贸rfico para SNNs.
    #pause
    - 1 mill贸n de neuronas LIF.
    #pause
    - 256 millones de conexiones entre neuronas (sinapsis).
    #pause
    - Pesos de 2 bits de precisi贸n.
    #pause
    - Dividido en 4096 cores.
    #pause
    - Salida binaria de neuronas (_"digital en amplitud"_).
    #pause
    - Actualizaci贸n asincr贸nica de neuronas (_"anal贸gico en tiempo"_).#footnote("T茅cnicamente tiene un tick de 1ms pero para muchos casos de usos puede aproximarse como anal贸gico.")
    #pause
    - Opcional: estocasticidad en la salida de las neuronas.
]


= 驴Por qu茅 usar SNNs?
#new-section-slide([], title: [])

#slide()[
    - _*Eficiencia energ茅tica*_: en muchos casos, el consumo de potencia es mucho menor que otras ANN principalmente debido a que:
        #pause
        - son _event-driven_: las neuronas solo transmiten informaci贸n (i.e.: un pulso) cuando ocurre un evento.
        #pause
        - las se帽ales de salida son poco densas (_sparse_) debido a que, para cada neurona, la mayor parte del tiempo no hay eventos.#footnote[Esto depende del entrenamiento, pero es posible incluir la cantidad de disparos total de la red como factor en la funci贸n de costo para regular el consumo de potencia total de la red.]
    #pause
    - _*Baja latencia*_:
        #pause
        - Como son _event-driven_, los eventos se propagan en el momento que ocurren.
        #pause
        - Como se comunican 煤nicamente con pulsos, todos los mensajes transmitidos son muy cortos.
]
#slide()[
    - _*Apropiadas para procesamiento series temporales*_: 
        - Como las neuronas tienen memoria de su entrada en el pasado, no hace falta una arquitectura recurrente para procesar temporales.
    #pause
    - _*Similitud a redes neuronales biol贸gicas*_: son m谩s cercanas a las redes neuronales biol贸gicas, lo que puede ser 煤til para modelar y estudiar su comportamiento.
]

== 驴Qu茅 es una c谩mara de eventos?
#focus-slide[Caso de uso: procesamiento de salida de c谩maras de eventos.]

#slide(title:"驴Qu茅 es una c谩mara de eventos?", composer: (1.3fr, 1fr))[
    #image("figs/camera_comparison.png")
][
    - C谩maras inspiradas en sistemas visuales biol贸gicos:
        #pause
        - Mayor sensibilidad a grandes cambios de intensidad que a intensidad constante.
        #pause
        - _Event-driven_: solo se guarda la informaci贸n de eventos (cambios). Donde no hay cambios no se guarda informaci贸n.
]

#slide(title:"驴Qu茅 es una c谩mara de eventos?", composer: (1.3fr, 1fr))[
    #image("figs/camera_comparison.png")
][
    - La salida es un arreglo de estructuras iguales con 3 datos (no se guardan frames completos!):
        - Posici贸n del evento
        - Timestamp del evento:
        - Polaridad del evento
    #pause
    #sym.arrow.double Comparada con una c谩mara de frames convencional, tiene:
    - Menos volumen de datos de salida
    - M谩s detalle del movimiento
]

#slide(align:center+horizon, composer: (15fr, 1fr))[
    #image("figs/frame_vs_event_based_rocks.png")][
    @Hendy_Merkel_2022
]

#slide(align:center+horizon, composer: (15fr, 1fr))[
    #image("figs/frame_vs_event_based_dataset.png", height: 80%)][
    @gesture_recognition_2017
]

== Procesamiento de salida de una c谩mara de eventos con SNNs
#slide(align: center+horizon)[
    #text(size: 24pt)[Por el formato de los datos de salida de una c谩mara de eventos,\
    las SNNs son ideales para procesarlos!]
]
#slide(align:center+horizon, composer: (15fr, 1fr))[
    #image("figs/gesture_recognition_arch.png")][
    @gesture_recognition_2017
]

#slide[
    - Resultados:
        - Latencia promedio de 105ms en reconocimiento de gestos (percibido como en tiempo real por un humano)
        - _Out-of-sample_ accuracy de entre 87.62% (para consumo de 88.5mW) y 96.44% (para consumo de 178.8mW) 
            #pause
            - #sym.arrow Un cargador de notebook de 60W alcanza para alimentar el an谩lisis de 300 streams de video en tiempo real.
]

= 驴C贸mo se entrenan las SNNs?
#new-section-slide([], title: [])

#slide()[
    _*Aprendizaje no supervisado*_
        #pause
        - Implementar reconfiguraci贸n de pesos basada en *regla de Hebb* / *regla de Oja*
            #pause
            - Requiere hardware que permita la reconfiguraci贸n de pesos. Truenorth no lo admite, pero existen otros que s铆 (ROLLS).\
][
    #pause
    _*Aprendizaje supervisado*_
        - Entrenar red neuronal tradicional y convertir a SNN (_shadow-training_).
        #pause
        - Entrenar con backpropagation adaptado para SNNs.
            #pause
            - ...驴C贸mo se adapta a la naturaleza no derivable de las SNNs? (_dead-neuron problem_)

]

== Backpropagation adaptado para SNNs

#slide(align: center+horizon)[
    #image("figs/training.png")
]
#slide[
    #image("figs/training_dead_neuron_detail.png")
    _Dead-neuron problem_
][
    #image("figs/training_surrogate_detail.png")
    \
    _Surrogate-gradient_
][
    #image("figs/backprop_eq.png")\
    Con _surrogate-gradient_, el t茅rmino B es finito y mayor a 0. #footnote([Siempre que haya un pulso de salida])
]

#set heading(outlined: false)
#slide()[
  #bibliography("refs.bib")
]

#focus-slide[Muchas gracias! ]


#show: appendix

= Leaky Integrate-and-Fire

#slide(title: "Leaky Integrate-and-Fire")[
  #definition(title: "Modelo Leaky Integrate-and-Fire (LIF)", blocktitle:"Definici贸n")[
  $
    cases(
      dot(s)(t) = 1/tau_s [s_("rest")-s(t)] + x(t),
      y(t) = sum_i^oo delta(t-t_f_i),
      s(t_(f_i) ^+) = s_("rest") ,
    )
  $ <eq:LIF>
  - $t_f_i$: $t$ tal que $s(t) >= s_(t h),$
  - $x(t)$: entrada (pulsos)
  - $s(t)$: estado de la neurona
  - $y(t)$: tren de deltas (disparos)
  ]
]

= SNN CNN
#slide(align: center)[
    #image("figs/SNNCNN.png")
]

= Regularizaci贸n

#slide[
    _*Cantidad m谩xima de pulsos*_:
        - Para regular el consumo de potencia
        - Aplica a la suma de pulsos de toda la poblaci贸n de neuronas de la red
    _*Cantidad m铆nima de pulsos*_:
        - Si hay neuronas que disparan muy poco, es muy dificil de entrenar.
        - Aplica a neuronas individuales.
]

